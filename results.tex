\section{Results}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{results/task_points.png}
  \caption{
    Examples did not noticeably affect time-to-completion, except in Task 2
    where the effect is dramatic.
  }
  \label{fig:data-points}
\end{figure*}
\subsection{Time-to-correct solution}
Cumulatively, across the four tasks, we did not observe a significant change
in the time-to-correct solution.
%
A Wilcoxon Rank-Sum test, performed on the aggregate of per-task rankings,
yielded a confidence interval $p=0.14$.
%
Performing rank-sum of ranks is an especially conservative test, because it weighs
per-task rankings equally, regardless of the absolute times.

Although the results do not indicate a change across the board, our data shows a dramatic
speedup for the example group in Task 2 (see Figure~\ref{fig:data-points}).
%
This may indicate that certain tasks benefit from examples while others do not.
%
We discuss this more thoroughly in Section~\ref{sec:discussion}

\subsection{Incorrect selections}
When participants made an incorrect selection, they were allowed to keep
selecting until they found the correct program.
%
This happened 3 times in the \noexamples group and 5 times in the example group.
%
In the example group, a single participant made 3 incorrect guesses on a
single task.
%
This participant was also the only one who ranked their Haskell knowledge as a 1 out of 5.
%
Futhermore, the incorrect selections contained incorrect input/output pairs. 
%
With these anomalies in mind, we don't believe this study has determined a definitive answer
on the relationship between examples and error rates.

\subsection{Interpreter usage}
In the \noexamples group, participants made a median of 6 type queries,
executed 3 subexpressions, and ran 3 full candidates across all 4 tasks.
%
In the \examples group, however, participants made a median of 0 queries of any kind.
%
It's clear that, introducing examples reduced participants reliance on the
interpreter to gather information.

% Figures.